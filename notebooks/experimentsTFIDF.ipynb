{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5255d43",
   "metadata": {},
   "source": [
    "# TFIDF Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689d0f8d",
   "metadata": {},
   "source": [
    "### Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75607bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbbfbca",
   "metadata": {},
   "source": [
    "### Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "871832cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\ironhack_bootcamp\\\\main-bootcamp\\\\week7\\\\fake-news-detection-nlp'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(r\"D:\\ironhack_bootcamp\\main-bootcamp\\week7\\fake-news-detection-nlp\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ee194c",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "367cedaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_headline</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>donald trump sends embarrass new year eve mess...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>drunk brag trump staffer start russian collusi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sheriff david clarke become internet joke thre...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trump obsess even obama name cod website image</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pope francis call donald trump christmas speech</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       news_headline  label\n",
       "0  donald trump sends embarrass new year eve mess...      0\n",
       "1  drunk brag trump staffer start russian collusi...      0\n",
       "2  sheriff david clarke become internet joke thre...      0\n",
       "3     trump obsess even obama name cod website image      0\n",
       "4    pope francis call donald trump christmas speech      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"dataset\\processed\\clean_dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe33b04c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "news_headline    0\n",
       "label            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfb75353",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7979a894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          donald trump sends embarrass new year eve message disturbing\n",
       "1        drunk brag trump staffer start russian collusion investigation\n",
       "2    sheriff david clarke become internet joke threaten poke people eye\n",
       "3                        trump obsess even obama name cod website image\n",
       "4                       pope francis call donald trump christmas speech\n",
       "Name: news_headline, dtype: str"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df['news_headline']\n",
    "y = df['label']\n",
    "\n",
    "pd.set_option('display.max_colwidth', 350)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7efec8",
   "metadata": {},
   "source": [
    "### Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2b3f9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train size: (27314,)\n",
      "X_test size: (6829,)\n",
      "y_train size: (27314,)\n",
      "y_test size: (6829,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"X_train size:\", X_train.shape)\n",
    "print(\"X_test size:\", X_test.shape)\n",
    "print(\"y_train size:\", y_train.shape)\n",
    "print(\"y_test size:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89406de",
   "metadata": {},
   "source": [
    "### TFIDF Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05cc3522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty DataFrame to store results\n",
    "metrics_df=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9110e51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.tfidf_vectorize import *\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "models = [\n",
    "    LogisticRegression(), \n",
    "    LinearSVC(), \n",
    "    MultinomialNB(), \n",
    "    RandomForestClassifier(n_estimators= 500, max_depth= 10), \n",
    "    XGBClassifier(n_estimators= 500, max_depth= 10)\n",
    "]\n",
    "ngram_vals = [(1, 1), (1, 2), (1, 3), (2, 2)]\n",
    "min_df_vals = [1, 10, 20]\n",
    "max_df_vals = [0.90, 0.95, 0.99]\n",
    "\n",
    "for model in models:\n",
    "    for n in ngram_vals:\n",
    "        for m in min_df_vals:\n",
    "            for j in max_df_vals:\n",
    "\n",
    "                row = run_tfidf_experiment(\n",
    "                    model=model,\n",
    "                    X_train=X_train,\n",
    "                    X_test=X_test,\n",
    "                    y_train=y_train,\n",
    "                    y_test=y_test,\n",
    "                    ngram_range=n,\n",
    "                    min_df=m,\n",
    "                    max_df=j,\n",
    "                    comments=f\"ngram={n}, min_df={m}, max_df={j}\"\n",
    "                )\n",
    "\n",
    "                metrics_df = pd.concat(\n",
    "                    [metrics_df, pd.DataFrame([row])],\n",
    "                    ignore_index=True\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13091b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.round(4).sort_values(by='test_f1', ascending= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16fd981",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509c7b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best config per model\n",
    "best_per_model =  metrics_df.sort_values(\"test_f1\", ascending=False).groupby(\"model\").first().reset_index()\n",
    "total_configs = len(ngram_vals) * len(min_df_vals) * len(max_df_vals)\n",
    "plt.figure(figsize=(8,5))\n",
    "\n",
    "bars = plt.bar(\n",
    "    best_per_model[\"model\"],\n",
    "    best_per_model[\"test_f1\"],\n",
    "    color = \"#C57827\",\n",
    "    width= 0.5\n",
    ")\n",
    "plt.title(f\"Best F1_test for per Model | {total_configs} Configs. tested per Model | TFIDF\", fontsize = '10')\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"Test F1\")\n",
    "plt.ylim(0.8, 1.0)\n",
    "plt.xticks(rotation=0, fontsize =9)\n",
    "\n",
    "# Add values to labels \n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width()/2,\n",
    "        height + 0.005,\n",
    "        f\"{height*100:.2f}%\",\n",
    "        ha='center',\n",
    "        va='bottom',\n",
    "        fontsize= 9\n",
    "    )\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c88f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best config per model\n",
    "best_per_model = (\n",
    "    metrics_df\n",
    "    .sort_values(\"test_f1\", ascending=False)\n",
    "    .groupby(\"model\")\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "models = best_per_model[\"model\"]\n",
    "scores = best_per_model[\"test_f1\"]\n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "\n",
    "bars = plt.bar(\n",
    "    models,\n",
    "    scores,\n",
    "    # hatch=\"///\",        # hatch pattern\n",
    "    color=\"#0e8530\",      # hex color (blue)\n",
    "    # edgecolor=\"black\"\n",
    "\n",
    ")\n",
    "\n",
    "plt.title(\"Best Test F1 per Model\")\n",
    "plt.xlabel(\"Models\")\n",
    "plt.ylabel(\"Test F1\")\n",
    "plt.ylim(0.8, 1.0)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add value labels + best parameters\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    \n",
    "    label_text = (\n",
    "        f\"F1={height:.3f}\\n\"\n",
    "        f\"ngram={best_per_model['ngram_range'][i]}\\n\"\n",
    "        f\"min_df={best_per_model['min_df'][i]}\\n\"\n",
    "        f\"max_df={best_per_model['max_df'][i]}\"\n",
    "    )\n",
    "    \n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width()/2,\n",
    "        height + 0.005,\n",
    "        label_text,\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=8\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
